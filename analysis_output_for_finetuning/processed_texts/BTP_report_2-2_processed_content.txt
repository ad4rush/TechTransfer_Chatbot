=== Page 1 ===
System-on-Chip design for AI Processing in Edge
Applications
Pravar Pathania : 2022376
Namit Gupta : 2022314
BTP report submitted in partial fulfillment of the requirements
for the Degree of B.Tech. in Electronics & VLSI Engineering and
B.Tech. in Electronics & Communications Engineering
on 23/04/2025
BTP Track: Research Track
BTP Advisor
Prof Sujay Deb
Indraprastha Institute of Information Technology
New Delhi

=== Page 2 ===
Student’s Declaration
We hereby declare that the work presented in the report entitled System-on-Chip design
for AI Processing in Edge Applications submitted by us for the partial fulfillment of the
requirements for the degree of Bachelor of Technology in Electronics & VLSI Engineering and
Bachelor of Technology in Electronics & Communications Engineering at Indraprastha Institute
of Information Technology, Delhi, is an authentic record of our work carried out under guidance
of Dr Sujay Deb Due acknowledgements have been given in the report to all material used.
This work has not been submitted anywhere else for the reward of any other degree.
.............................. Place & Date: Delhi, April 23, 2025
Pravar Pathania
..............................
Namit Gupta
Certificate
This is to certify that the above statement made by the candidate is correct to the best of my
knowledge.
.............................. Place & Date: Delhi, April 23, 2025
Dr Sujay Deb
2

=== Page 3 ===
Abstract
This research addresses the critical challenges of implementing deep learning algorithms on
resource-constrained edge devices through the development of a specialized System-on-Chip
(SoC) design. As AI applications increasingly migrate from cloud infrastructures to edge de-
viceslikesmartphones,IoTsensors,anddrones,conventionalalgorithm-specificacceleratorslack
theadaptabilityrequiredfordiverseAIworkloadsunderstrictpower, memory, andcomputation
constraints.
Our work presents a comprehensive approach to edge AI hardware design through three comple-
mentarycomponents: acustomCNNaccelerator,anIbex-basedRISC-VSoC,andaCNN+GRU
neuralnetworkpipelineforvideo-basedcrowdanalysis. TheCNNacceleratorimplementsanovel
quadrant-based processing architecture with a 4×4 processing element array, achieving approx-
imately 9.1 GOPs at 100MHz with efficient resource utilization (93% BRAM, 74% LUT). This
quadrant-based design significantly reduces memory access conflicts and improves throughput
for spatial feature extraction. The SoC architecture integrates this accelerator through AXI-
DMA controllers for efficient bulk data movement between DDR memory and computational
elements. System validation combines functional testing on a Xilinx Zedboard FPGA with al-
gorithmic evaluation on the TUB CrowdFlow dataset, demonstrating 93% validation accuracy
for crowd motion pattern classification.
Key innovations include hierarchical memory organization, parallelized computation across im-
age quadrants, and balanced resource allocation prioritizing data movement efficiency over
raw computation. Future enhancements will incorporate compressed sparse computation, zero-
skipping logic, and configurable precision scaling to further optimize the architecture for emerg-
ing edge AI applications, where balancing performance with environmental sustainability con-
cerns becomes increasingly important.
Keywords: Deep Learning Accelerator, Edge AI, Reconfigurable Hardware, Convolution Neural
Networks(CNN), Gated Recurrent Unit(GRU)

=== Page 4 ===
Contents
1 Introduction 1
1.1 Edge-AI Challenges and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.1 Constraints in Edge Deployment . . . . . . . . . . . . . . . . . . . . . . . 2
1.1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Project Goals and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1 Custom CNN Accelerator . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.2 Preliminary Ibex-Based RISC-V SoC . . . . . . . . . . . . . . . . . . . . . 3
1.2.3 End-to-End CNN+LSTM Pipeline . . . . . . . . . . . . . . . . . . . . . . 3
2 Background & Related Work 4
2.1 Evolution of Edge AI Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Reconfigurable CNN Accelerators . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2.1 Architectural Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2.2 Sparsity and Quantization Exploitation . . . . . . . . . . . . . . . . . . . 5
2.3 Heterogeneous RISC-V SoCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3.1 Open-Source RISC-V Ecosystem . . . . . . . . . . . . . . . . . . . . . . . 6
2.3.2 Accelerator Integration Approaches. . . . . . . . . . . . . . . . . . . . . . 6
2.3.3 Memory Hierarchy Considerations . . . . . . . . . . . . . . . . . . . . . . 7
3 CNN Accelerator Architecture 8
3.1 Memory Hierarchy and Data Organization . . . . . . . . . . . . . . . . . . . . . . 8
3.1.1 Memory Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.1.2 Quadrant-Based Data Organization . . . . . . . . . . . . . . . . . . . . . 9
3.1.3 Data Transfer Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Processing Element Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2.1 PE Organization and Computation Model . . . . . . . . . . . . . . . . . . 10
3.2.2 PE Computational Units . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2.3 Weight Storage and Management . . . . . . . . . . . . . . . . . . . . . . . 11
3.3 Controller Architecture and Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.3.1 PE Controller Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.3.2 Window Management and Slot Allocation . . . . . . . . . . . . . . . . . . 12
3.4 Performance Analysis and Optimization . . . . . . . . . . . . . . . . . . . . . . . 13
3.4.1 Computational Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1

=== Page 5 ===
3.4.2 Memory Bandwidth Considerations . . . . . . . . . . . . . . . . . . . . . 13
3.4.3 Resource Utilization and Optimizations . . . . . . . . . . . . . . . . . . . 13
3.5 Implementation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.5.1 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.5.2 Synthesis Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.5.3 Scalability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.5.4 Comparison with State-of-the-Art . . . . . . . . . . . . . . . . . . . . . . 15
3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4 Algorithm Implementation and Evaluation 17
4.1 Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.2 Dataset Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.3 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.3.1 Custom CNN Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.3.2 GRU Sequence Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4 Training Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4.1 Data Loading and Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4.2 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4.3 Optimization and Regularization . . . . . . . . . . . . . . . . . . . . . . . 19
4.5 Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5 SoC Architecture 20
5.1 Current SoC Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.1.1 Foundational Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.1.2 Memory Subsystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.1.3 Peripheral Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.1.4 Interconnect System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2 Proposed SoC with AXI DMA Integration . . . . . . . . . . . . . . . . . . . . . . 21
5.2.1 AXI DMA Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.2.2 Enhanced Interconnect Architecture . . . . . . . . . . . . . . . . . . . . . 21
5.2.3 Memory Subsystem Enhancements . . . . . . . . . . . . . . . . . . . . . . 21
5.2.4 Software Stack Modifications . . . . . . . . . . . . . . . . . . . . . . . . . 22
5.3 Implementation Status and Verification . . . . . . . . . . . . . . . . . . . . . . . 22
5.3.1 Implementation Status . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6 Future Directions 23
2

=== Page 6 ===
Chapter 1
Introduction
Edge computing is rapidly reshaping how we design hardware for artificial intelligence. Instead
of running AI tasks in centralized data centres, we move many inference workloads directly onto
small, power-limited devices at the network’s edge. This shift is not just a minor tweak—it calls
for a complete rethink of how hardware and software work together, especially when we must
balance power and performance with strict area restrictions.
Last semester, we analyzed two leading deep-learning accelerators—Eyeriss V2 [2] for convo-
lutional networks and EdgeDRNN [3] for gated recurrent units—to blend their strengths into
one flexible design. We studied how CNNs perform highly parallel, data-dense operations while
GRUs rely on step-by-step, time-dependent processing. We proposed a theoretical, reconfig-
urable architecture that handles both workloads from that analysis.
Inthecurrentsemester, wehavemovedfromtheorytopractice. Wedesignedandbuiltacustom
CNNacceleratortunedforedgedevices. WeplantointegrateitintoacustomIbex-basedRISC-
VSoC,demonstratingitscapabilitieswithareal-worldcrowd-detectionpipelinecombiningCNN
andLSTMmodels. OurworkmattersbecauseedgeAIiseverywhere—fromself-drivingcarsand
factory sensors to wearable health monitors and smart cities. As deployments grow, the need for
adaptable, energy-efficient accelerators becomes urgent. At the same time, the environmental
cost of designing and manufacturing new silicon must be accounted for alongside traditional
metrics like speed and area.
This report presents our design choices, trade-offs, and test results for the custom CNN accel-
erator and the plan for SoC integration.
1

=== Page 7 ===
1.1 Edge-AI Challenges and Motivation
1.1.1 Constraints in Edge Deployment
Deploying deep-learning models on edge devices forces a clash between complex algorithms and
tight hardware budgets. Devices such as drones, IoT sensors, smartphones, and wearables have:
• Limited compute resources
• On-chip memory in the kilobyte range
• Thermal caps around 1–2 W
• Battery budgets measured in millijoules per inference
1.1.2 Motivation
CNNs benefit from highly parallel, spatially uniform operations, well-suited to systolic arrays
with weight-stationary dataflows and extensive data reuse. In contrast, GRUs and LSTMs
involve sequential, irregular operations with lower arithmetic intensity, favoring deeper memory
hierarchies.
Traditional ASICs like Google’s TPU or NVIDIA’s Tensor Cores optimize specific tensor opera-
tionsbutlackflexibility. Additionally, siliconfabricationincurshighenvironmentalcosts—often
exceeding the operational energy consumption of a device over its lifetime. With billions of edge
devices projected, carbon accountability in hardware design becomes critical.
Our previous semester’s theoretical accelerator combined Eyeriss V2’s hierarchical mesh NoC
withEdgeDRNN’sdelta-drivenoptimizations. Simulationsshowedpotential,butpracticalissues
like power delivery, memory bandwidth, and software integration required a shift to hands-on
development.
1.2 Project Goals and Scope
ThisprojectaimstodevelopacompleteedgeAIsystemcombiningacustomCNNaccelerator, a
lightweight RISC-V SoC based on the Ibex core, and a novel CNN+LSTM algorithm for video-
based crowd analysis. The system is designed for quantized, low-power inference and real-time
performance on resource-constrained devices.
1.2.1 Custom CNN Accelerator
AreconfigurableCNNacceleratorhasbeendesignedtosupportmodernlightweightarchitectures
such as MobileNet and EfficientNet, focusing on flexibility and efficiency.
2

=== Page 8 ===
Key Features:
• Reconfigurable PE Array: Supports efficient 3 × 3 depthwise and 1 × 1 pointwise
convolutions.
• Multi-BankScratchpadMemory: Enableslocalizedstorageforactivationsandweights,
reducing external memory access.
1.2.2 Preliminary Ibex-Based RISC-V SoC
A basic SoC infrastructure has been developed using the Ibex RISC-V core, providing the
foundational components for full system integration. The current setup includes peripheral
support and a central interconnect for future attachment of the accelerator and DMA controller.
System Features:
• Core: Ibex RISC-V core used for control and general purpose processing.
• AHB Interconnect: Central interconnect to facilitate communication between the core,
peripherals, and future accelerator modules.
• Peripherals: UART, I2C and GPIO integrated for communication and control.
• Planned DMA Integration: A DMA engine is planned to transfer feature maps and
weights from DDR memory to the CNN accelerator.
• Accelerator Interface: Memory-mapped register space reserved for future attachment
of the custom CNN accelerator.
• FPGA Validation: BasicSoCtestedonXilinxZynq-7000,includingsuccessfulbitstream
generation and peripheral functionality.
1.2.3 End-to-End CNN+LSTM Pipeline
To demonstrate the practical utility of our system, we have developed a CNN+LSTM pipeline
aimed at video-based crowd counting.
Pipeline Overview:
• CNN Frontend: Performs spatial feature extraction.
• LSTM Backend: Models temporal dependencies using frame sequences.
• 8-bit Quantization: Matches the fixed-point capabilities of the accelerator hardware.
• Task Partitioning: CNN operations are intended to run on the accelerator, while LSTM
computation is handled by the Ibex core.
• Target Benchmark: Achieving > 93% F1-score on the TUB Crowdflow dataset.
3

=== Page 9 ===
Chapter 2
Background & Related Work
2.1 Evolution of Edge AI Architectures
The paradigm shift toward edge computing for artificial intelligence workloads represents a
fundamental rethinking of how computational resources are deployed. Rather than relying
solely on cloud infrastructure, edge AI moves inference—and increasingly, training—closer to
data sources, addressing several critical challenges:
• Latency Constraints: Real-time applications like autonomous vehicles and industrial
automation require millisecond-level responses.
• Bandwidth Limitations: Transmittingrawsensordatatocloudserversisoftenimprac-
tical due to network constraints.
• Privacy Concerns: Localprocessingeliminatestheneedtotransmitpotentiallysensitive
data.
• Energy Efficiency: Reducing transmission costs can significantly lower overall system
energy consumption.
• Reliability Requirements: Edge processing maintains functionality during network
outages.
Thisshifthasdriventhedevelopmentofspecializedhardwarearchitecturesthatbalancecompu-
tational capability, energy efficiency, and area constraints. The evolution of these architectures
spans multiple generations:
1. CPU-Based Systems: Initial edge implementations used general-purpose processors
with software optimizations.
2. GPU Acceleration: Embedded GPUs provided parallel processing capabilities at mod-
erate power levels.
3. Custom Accelerators: Purpose-built ASICs and FPGA solutions targeting specific AI
workloads.
4. Heterogeneous SoCs: Integrated platforms combining low-power CPUs with domain-
specific accelerators.
4

=== Page 10 ===
Our work positions itself in this fourth category, leveraging the flexibility of RISC-V alongside
custom acceleration to create a balanced system for edge deployment.
2.2 Reconfigurable CNN Accelerators
The push to run increasingly complex CNNs at the edge has spurred the development of recon-
figurable accelerators that balance high performance with architectural flexibility in constrained
environments.
2.2.1 Architectural Paradigms
CNN accelerator designs typically optimize for one of several dataflow patterns that determine
how data moves through the system:
• Weight-Stationary: Minimizes weight movement by fixing weights at PEs while activa-
tions flow through.
• Output-Stationary: Minimizespartialsummovementbyaccumulatingresultsatoutput
locations.
• Row-Stationary: Optimizes for both convolutional reuse and data movement energy.
• Input-Stationary: Keeps input activations fixed while weights are broadcast.
Recentstate-of-the-artacceleratorsoftenimplementhybridapproachesthatadapttolayerchar-
acteristics. Eyeriss V2 pioneered this with its two-level Hierarchical Mesh Network-on-Chip
(HM-NoC), which dynamically switches among unicast, multicast, and broadcast modes to
maximize data reuse opportunities and reduce congestion.
2.2.2 Sparsity and Quantization Exploitation
Modern edge AI accelerators increasingly leverage two complementary optimization techniques:
Sparse Computation
• CSC Format: Compressed Sparse Column storage for efficient weight representation.
• Zero-Skipping: Hardware mechanisms to detect and bypass zero-valued operations.
• Dynamic Activation Pruning: Runtime identification of unimportant activations.
5

=== Page 11 ===
Low-Precision Computation
• 8-bit Quantization: Dominantapproachforinference,balancingaccuracyandefficiency.
• Mixed-Precision: Using different bit widths for different operations or network layers.
• Bit-Serial Processing: Computing results progressively bit-by-bit for flexible precision.
The UNPU (Ultra-low-power Neural Processing Unit) exemplifies precision scaling with bit-
serial PEs, achieving over 50 TOPS/W at 1-bit precision. Similarly, Google’s Edge TPU targets
8-bit quantized inference with 4 TOPS at under 2 W, emphasizing predictable performance
through static quantization.
2.3 Heterogeneous RISC-V SoCs
Integrating domain-specific accelerators alongside general-purpose cores is central to modern
edge SoC design. RISC-V’s open, modular ISA makes it an ideal host for such heterogeneous
systems.
2.3.1 Open-Source RISC-V Ecosystem
The RISC-V instruction set architecture has emerged as an ideal foundation for heterogeneous
computing platforms due to several key advantages:
• Modularity: The base ISA can be extended with optional instruction sets tailored to
specific workloads.
• Openness: The open standard eliminates licensing barriers and encourages innovation.
• Scalability: Implementations range from microcontrollers to high-performance comput-
ing.
• Verification: Formalverificationeffortshaveincreasedreliabilityforcriticalapplications.
The Ibex core, developed as part of the PULP platform by ETH Zurich and now maintained
by lowRISC, exemplifies these advantages. With a small footprint (26.6 kGE in its minimal
configuration) and moderate performance (2.47 CoreMark / MHz), it is well suited for control
tasks in edge devices.
2.3.2 Accelerator Integration Approaches
Integrating specialized accelerators with RISC-V cores typically follows one of three paradigms:
6

=== Page 12 ===
1. Memory-MappedI/O:Acceleratorsappearasmemory-mappeddeviceswithcontrol/status
registers.
• Advantages: Simple programming model, compatibility with existing software.
• Disadvantages: Higher latency for fine-grained control.
2. Custom Instruction Extensions: Embeddingacceleratorfunctionalitydirectlyintothe
core’s pipeline.
• Advantages: Lower invocation overhead, tighter coupling.
• Disadvantages: Complexity in verification, potential ISA fragmentation.
3. Shared Memory Clusters: Multi-core clusters with shared memory and specialized
units.
• Advantages: Parallelism across multiple cores, flexibility in task allocation.
• Disadvantages: Coherency complexity, programming challenges.
Our design adopts memory-mapped integration for the CNN accelerator, enhanced with a co-
herent AXI-4 DMA controller to mitigate latency and maximize throughput for bulk data move-
ment.
2.3.3 Memory Hierarchy Considerations
Edge AI SoCs require careful memory hierarchy design to balance capacity, bandwidth, and
energy efficiency:
• Scratchpad Memories: Software-managed buffers offering predictable timing and en-
ergy consumption.
• DMA Controllers: Efficient bulk data movement reducing CPU overhead.
• Multi-Bank Organizations: Parallelaccesstomultiplememorybankstoincreaseband-
width.
• Hierarchical Caching: Sized appropriately for workload locality characteristics.
Recent studies show that memory bandwidth, rather than raw compute, often limits accelerator
performance at the edge. By incorporating a high-bandwidth AXI DMA controller into our
system, we alleviate this critical bottleneck, enabling full utilization of our CNN accelerator’s
computational resources.
7

=== Page 13 ===
Chapter 3
CNN Accelerator Architecture
This section details the design of our CNN accelerator architecture implemented in HLS (High-
Level Synthesis) targeting Zedboard. The design features a quadrant-based processing ap-
proach with a 4×4 PE array capable of delivering approximately 9.12 GOPs at 100MHz for
MobileNetV2’s initial 3×3 convolution layer (224×224×3 to 112×112×32). The architecture
efficiently handles memory access patterns through a hierarchical buffer system and processes
different image regions in parallel.
The architecture consists of several hierarchically organized computational units:
• DDR to BRAM Controller: Interfaces with external memory and manages data dis-
tribution.
• Input/Output BRAM Modules: Intermediate buffer storage organized by quadrants.
• PE Controller: Orchestrates data movement between memory units and processing ele-
ments.
• PE Cluster: Core computational engine consisting of a 4×4 grid of Processing Elements.
3.1 Memory Hierarchy and Data Organization
3.1.1 Memory Architecture
The accelerator implements a four-level memory hierarchy:
• External DDR Memory: High-capacity storage for input/output feature maps (64-bit
word width).
• Output BRAMs: On-chipbufferstoragedividedintofourquadrants(24-bitwordwidth)
stores the initial inputs and temporary outputs for each layer.
• Input BRAMs: Hold small windows just before the PE Cluster to hold the immediate
data the PE’s need to process(96-bit word width).
• PE Local Memory: Weight storage and computation scratchpads within each PE.
Thishierarchicalapproachbalancescapacity,bandwidth,andaccesslatencyrequirementsacross
different stages of computation.
8

=== Page 14 ===
Figure 3.1: CNN Accelerator Architecture
3.1.2 Quadrant-Based Data Organization
A key innovation in this architecture is the quadrant-based organization of feature maps. For a
standard 224×224×3 input image:
• Q0 (Top-Left): rows 0–111, columns 0–111
• Q1 (Top-Right): rows 0–111, columns 112–223
• Q2 (Bottom-Left): rows 112–223, columns 0–111
• Q3 (Bottom-Right): rows 112–223, columns 112–223
EachquadrantisstoredinadifferentoutputBRAMandprocessedbyadedicatedrowinthePE
array, enabling parallel computation while maintaining locality of reference. This approach sig-
nificantly reduces memory access conflicts and improves computational throughput. Additional
details on the output BRAM organization:
• Word Width: Each output BRAM has a 24-bit word (composed of three concatenated
8 bit pixels).
• Module Count: Four output BRAM modules exist, one for each row of the PE array.
Additional details on the input BRAM organization:
9

[Image page_14_image_0.jpeg Analysis (by Gemini)]
Here's a thorough analysis of the image based on the provided context:

**Visual Elements and Structure:**

*   **Diagram:** The image is a block diagram illustrating the architecture of a CNN (Convolutional Neural Network) accelerator.
*   **Blocks:** The diagram uses different colored blocks to represent functional units within the accelerator.
    *   **PE Controller (Pink):** A central rounded rectangle labeled "PE CONTROLLER".
    *   **Input BRAMs (Salmon):** Four rectangular blocks, each labeled "INPUT BRAM", arranged in a row beneath the PE Controller.
    *   **Processing Elements (PEs) (Light Blue):** A 4x4 grid of rectangular blocks, each labeled "PE". This is the core computational engine.
    *   **Output BRAMs (Yellow):** Four rectangular blocks, each labeled "OUTPUT BRAM", arranged in a column to the right of the PE grid.
    *   **DDR to BRAM Controller (Light Green):** A large rectangular block on the far right, labeled "DDR TO BRAM CONTROLLER".
*   **Arrows/Lines:** Arrows indicate the flow of data and control signals between these units.
    *   **Red Arrows:** Connect the PE Controller to the Input BRAMs, and the Input BRAMs to the rows of the PE array. This likely represents the input data being fed into the PEs for processing.
    *   **Gold Arrows:** Connect the PEs to the Output BRAMs. This represents the outputs from each PE being stored.
    *   **Black Arrows:** Connect the PE Controller to the DDR to BRAM Controller, and the Output BRAMs to the DDR to BRAM Controller, these connections indicate the passing of parameters and output feature maps for the next layer.
*   **Layout:** The diagram has a hierarchical layout, showing data flowing from left to right.

**Text within the Image:**

*   "PE CONTROLLER"
*   "INPUT BRAM" (repeated four times)
*   "PE" (repeated sixteen times)
*   "OUTPUT BRAM" (repeated four times)
*   "DDR TO BRAM CONTROLLER"

**Significance and Context:**

Based on the surrounding text and the image itself, the following points are crucial:

*   **CNN Accelerator Architecture:** The diagram depicts a hardware accelerator designed for CNNs, specifically targeting MobileNetV2's initial convolutional layer. The surrounding text indicates that this accelerator is implemented in HLS (High-Level Synthesis) for the Zedboard platform.
*   **Quadrant-Based Processing:** A key feature is the "quadrant-based organization of feature maps" (mentioned in Figure 3.1's caption and text). The four Input BRAMs are associated with this quadrant division. The text explains that a 224x224 input image is divided into four quadrants, each stored in a separate Output BRAM and processed by a dedicated row in the PE array.
*   **Parallel Computation:** The architecture is designed for parallel computation. Each of the four rows of the PE array operates on a different quadrant simultaneously. This parallelism aims to improve computational throughput.
*   **Memory Hierarchy:** The text emphasizes a four-level memory hierarchy. The Input BRAMs act as on-chip buffer storage and are used as a buffer before sending to the PE.
*   **Data Flow:** Data is transferred from external DDR memory (via the DDR to BRAM Controller) into Input BRAMs. The PE Controller likely manages the flow of data from the Input BRAMs into the PE array. The PEs perform the convolutional operations, and the results are stored in the Output BRAMs and transferred again to the DDR memory through the DDR to BRAM Controller.
*   **Processing Element (PE):** The 4x4 PE array is at the heart of the accelerator. Each PE likely performs a 3x3 convolution with 3 input channels and can handle up to 32 output channels. Each row of the PE array processes a different quadrant of the input image.

**Overall Importance:**

The image (Figure 3.1) is essential for understanding the architecture of the CNN accelerator being presented in the research paper. It visually communicates the data flow, the interaction between different memory modules (DDR, Input BRAM, Output BRAM, and the implied PE local memory), and the parallel processing capabilities of the PE array. The diagram, combined with the text, provides a clear picture of how the quadrant-based data organization is implemented to improve memory access and computational performance. This architecture is designed to accelerate the computationally intensive convolutional layers of CNNs on embedded platforms.



=== Page 15 ===
• Word Width: Each input BRAM has a 96-bit word (composed of four concatenated
24-bit words).
• Module Count: Four input BRAM modules exist, one for each column of the PE array.
• Window Storage: Each input BRAM can store two windows, where one window holds
data for a full 3×3 kernel.
3.1.3 Data Transfer Mechanisms
The DDR to BRAM controller implements efficient burst-oriented transfers, with specialized
handling for pixel organization.
• Burst Optimization: Utilizes HLS pragma directives to enable burst transfers with up
to 256 words and supports 4 outstanding read requests.
• Unpacking Strategy: Converts 64-bit DDR words (containing two RGB pixels) into
24-bit BRAM words (one RGB pixel each).
• Routing: Routes unpacked data to the appropriate quadrant BRAM based on pixel
location.
3.2 Processing Element Architecture
3.2.1 PE Organization and Computation Model
Each Processing Element (PE) is designed to efficiently execute 3×3 convolutions with 3 input
channels and up to 32 output channels. The PE array consists of 16 PEs arranged in a 4×4
grid:
• Row Assignment: Each row processes a different quadrant of the input image.
• Column Assignment: Each column handles different spatial regions within the quad-
rants.
3.2.2 PE Computational Units
Each PE performs convolution over a 3×3 receptive field with three input channels (R, G, B)
and supports up to 32 output channels. The computation involves the following steps:
• Weight Memory Initialization: Each PE maintains a local 4D weight array indexed
by output channel, input channel, and kernel dimensions.
10

=== Page 16 ===
• Parallel Multiplication: Input pixels for each kernel position are unpacked into indi-
vidual RGB channels. These are multiplied in parallel with the corresponding weights for
each output channel.
• Fixed-Point Accumulation: ProductsfromeachRGBchannelaresummedusing32-bit
accumulators to preserve numerical precision.
• Activation Clipping and Packing: After accumulation, results are clipped to 8-bit
values and packed into the final output format.
The architecture leverages several optimization strategies to improve throughput and resource
efficiency:
• Complete Weight Array Partitioning: Enablesparallelaccesstoweightsacrosschan-
nels for efficient computation.
• Sequential Output Channel Processing: Balanceshardwareutilizationbycomputing
one output channel at a time.
• Explicit Bit Manipulation: Unpacks RGB values from a single 24-bit pixel input to
enable efficient per-channel operations.
3.2.3 Weight Storage and Management
Each PE contains local weight memory organized as a 4-dimensional array:
• Output Channels: 32
• Input Channels: 3 (RGB)
• Filter Dimensions: 3×3
This arrangement supports efficient access patterns during convolution and keeps the memory
footprint manageable.
3.3 Controller Architecture and Dataflow
3.3.1 PE Controller Design
The PE Controller orchestrates data movement and computation across the accelerator by im-
plementing a finite state machine with multiple operational states:
• IDLE: Waits for a start signal to begin processing.
11

=== Page 17 ===
• CHECK SLOT: Identifies available slots in the input BRAMs.
• CHECK CONTENTION: Resolves potential access conflicts for the output BRAMs.
• READ PIXELS: Initiates pixel read operations from the output BRAMs.
• WAIT DATA: Waits for the arrival of pixel data, handling any timing variations.
• WRITE INPUT BRAM: Assembles window data and writes it to the input BRAMs.
• NOTIFY WINDOW COMPLETE: Signals completion of the current window’s data
transfer.
• DONE: Indicates that all windows have been processed.
A stride-based window sliding mechanism is used to downsample feature maps efficiently. The
controller updates its position across the image using a stride of 2 pixels, moving to the next
row once the current row is fully processed. Upon reaching the end of the image, the controller
transitions to the DONE state.
3.3.2 Window Management and Slot Allocation
A critical feature of the architecture is its dynamic window management system. Each input
BRAM includes two window slots, enabling concurrent data loading and computation through
pipelining.
Slot statuses are managed using a simple classification scheme:
• SLOT FREE: The slot is available for new data.
• SLOT OCCUPIED: The slot holds valid data that has not yet been processed.
• SLOT PROCESSING: The data in the slot is currently being used by the processing
elements.
The controller uses a demand-driven approach to allocate BRAM slots. It scans for the most
suitable slot starting from the current target and iterates through the BRAM modules in a
round-robin fashion. If no slot is available in the current index, it proceeds to check alternate
indices. This method ensures:
• High Hardware Utilization: Keeps the PE array active by avoiding idle cycles.
• Conflict Avoidance: Prevents contention by intelligently distributing data across avail-
able slots.
• Efficient Resource Scheduling: Minimizeslatencyinmemoryaccessanddatatransfer.
12

=== Page 18 ===
3.4 Performance Analysis and Optimization
3.4.1 Computational Throughput
The accelerator’s performance is evaluated in terms of multiply-accumulate operations (MACs).
For a single 3×3 window with 3 input channels and 32 output channels:
• Total PE 4×4 = 16
• MACs per PE: 3
• Total MACs: 48
Assuming a clock frequency of 100MHz and ideal conditions, the theoretical peak performance
is approximately:
48×100,000,000
= 4.8 GMACs
1,000,000,000
4.8GMACs = 9.6GOPs
Actual performance is constrained by memory bandwidth, synchronization delays, and control
overhead.
3.4.2 Memory Bandwidth Considerations
The architecture includes several strategies to optimize memory bandwidth usage:
• Quadrant-based processing: Distributes memory accesses across quadrants to reduce
contention.
• Window buffering: Enables data reuse, lowering memory traffic.
• Parallel BRAM access: Facilitates simultaneous access to data across different quad-
rants.
• DDR burst transfers: Exploits high-throughput bursts for bulk data movement.
These techniques collectively reduce bottlenecks and improve throughput efficiency.
3.4.3 Resource Utilization and Optimizations
To balance throughput and resource usage, the design leverages:
13

=== Page 19 ===
• Selective weight array partitioning: Enables parallel computation across channels.
• Buffer partitioning: Supports concurrent data access and processing.
• Efficient pipelining: Maintains a continuous flow of data through the accelerator.
This balance makes the design practical for mid-range FPGA platforms such as the Zedboard.
3.5 Implementation Results
3.5.1 Performance Metrics
Experimental results from a testbench simulation indicate the following:
• Processing rate: ∼ 9.1 GOPs at 100MHz
• Processing time (224×224×3 input): ∼3ms (excluding DDR transfer)
3.5.2 Synthesis Results
The synthesis results from Vivado HLS 2019.1 for the CNN accelerator implementation are
presented in Figure 3.2. The resource utilization summary provides detailed insights into the
hardware footprint of the design.
Figure 3.2: Resource utilization summary from Vivado HLS showing the distribution of hardware re-
sources across different components of the CNN accelerator.
The utilization estimates indicate efficient resource allocation across different FPGA compo-
nents:
14

[Image page_19_image_0.png Analysis (by Gemini)]
Here's a comprehensive analysis of the image provided, based on the surrounding context from your research paper:

**Overall Context and Importance:**

The image, labeled "Figure 3.2: Resource utilization summary from Vivado HLS showing the distribution of hardware resources across different components of the CNN accelerator," presents a crucial aspect of the FPGA-based CNN accelerator's implementation. The preceding text emphasizes that the design aims for a balance between computational throughput and resource usage, making it practical for mid-range FPGA platforms.  The "Implementation Results" section explicitly states that Figure 3.2 provides detailed insights into the hardware footprint of the design based on synthesis results from Vivado HLS 2019.1.

The surrounding text reveals the focus on memory bandwidth optimization through techniques like quadrant-based processing, window buffering, parallel BRAM access, and DDR burst transfers. The emphasis on high BRAM utilization (close to the available resource) and relatively low DSP usage reinforces the design philosophy of a memory-centric CNN accelerator optimized for efficient data movement rather than intense computation.

**Visual Elements and Structure:**

The image is a screenshot of the Vivado HLS (High-Level Synthesis) environment. The key element is a table presenting the "Utilization Estimates" after synthesis.  It's a summary report breaking down the usage of different hardware resources on the FPGA.

*   **Table Structure:** The table has the following columns:

    *   **Name:** Identifies the resource type or component.
    *   **BRAM_18K:** Number of 18Kb Block RAM units utilized.
    *   **DSP48E:** Number of Digital Signal Processing (DSP) slices utilized.
    *   **FF:** Number of Flip-Flops utilized.
    *   **LUT:** Number of Look-Up Tables utilized.
    *   **URAM:** Number of UltraRAM units utilized.

The rows are categorized as follows: DSP, Expression, FIFO, Instance, Memory, Multiplexer, Register, Total, Available, and Utilization (%).

The left-hand panel shows a hierarchical project structure within the Vivado HLS environment. The right-hand panel shows the project outline, from which we can navigate to view the project details.

**Text within the Image and their Significance:**

*   **Vivado HLS 2019.1**: Specifies the software version used for the synthesis.
*   **BTP_work_final**: Project name.
*   **cnn_accelerator_top_csynth.rpt**: Indicates the specific report being displayed (CNN accelerator's top-level synthesis report).
*   **Utilization Estimates Summary Table:**

    *   **Name**: This column lists the hardware resources being used.
        *   DSP: Resources related to Digital Signal Processing.
        *   Expression: Logic expressions
        *   FIFO: First-In, First-Out memory buffers.
        *   Instance: Individual components or processing elements.
        *   Memory: Memory blocks.
        *   Multiplexer: Multiplexing logic.
        *   Register: Registers.
    *   **Resource Columns (BRAM_18K, DSP48E, FF, LUT, URAM)**: Shows the number of each resource type used by the corresponding component. The final rows, "Total" and "Available" show the total resources used and the total available resources on the target FPGA, respectively.

*   **Utilization Percentages**: Shows the percentage of available resources utilized. This is crucial for assessing the efficiency of the design.

    *   BRAM_18K: 93% utilized
    *   DSP48E: 3% utilized
    *   FF: 26% utilized
    *   LUT: 74% utilized
    *   URAM: 0% utilized

**Interpretation and Importance:**

The key findings from the table are:

*   **High BRAM Utilization (93%):** This confirms the memory-intensive nature of the CNN accelerator, aligning with the architecture's focus on optimizing memory bandwidth, as highlighted in the text.
*   **Low DSP Usage (3%):** This reinforces that the design isn't compute-bound. The text mentions achieving competitive performance with only 8 DSP units compared to other implementations. The design prioritizes data movement over heavy computation, making it efficient on FPGA platforms with limited DSP resources.
*   **Significant LUT Usage (74%):**  This reflects the complexity of the combinational logic required for data processing elements, pipeline control, and the quadrant-based architecture's data routing (as mentioned in the text).
*   **Moderate FF Usage (26%):** Indicates a balanced utilization for registers, control logic, and pipeline stages within the accelerator.
*   **URAM Usage (0%):** URAM are relatively new compared to other types of resources on an FPGA, hence their utilization would have been a later optimization after exhausting BRAM first.

The image, therefore, provides quantitative validation for the design choices described in the research paper. It demonstrates how resource allocation was carefully managed to achieve high performance while remaining practical for mid-range FPGA platforms. The figure serves as supporting evidence for the claims made about memory-centric design and efficient data movement, rather than heavy computation.



=== Page 20 ===
• BRAM 18K: 261 units (93% utilization), primarily allocated for memory buffers and
FIFOs, demonstrating the memory-intensive nature of CNN operations.
• DSP48E: 8 units (only 3% utilization), indicating that the design is not compute-bound
but rather optimized for memory access patterns.
• FF (Flip-Flops): 27,844 units (26% utilization), used for registers, control logic, and
pipeline stages within the accelerator.
• LUT (Look-Up Tables): 39,534 units (74% utilization), representing the combinational
logic resources used for implementing the processing elements and dataflow control.
The detailed breakdown shows that memory components (including BRAMs and FIFOs) con-
stitute a significant portion of the resource utilization, accounting for 1,314 FF and 2,814 LUT
resources. The multiplexer logic requires 3,015 LUTs, highlighting the importance of data rout-
ing in the quadrant-based architecture. The instance components, which include the processing
elements, utilize 261 BRAMs, 8 DSPs, 27,844 FFs, and 39,534 LUTs.
Overall, the synthesis results suggest that the design achieves a high BRAM utilization (93%)
and significant LUT usage (74%), while maintaining moderate FF utilization (26%). This re-
source distribution aligns well with the design philosophy of a memory-centric CNN accelerator
optimized for efficient data movement rather than compute-intensive operations.
3.5.3 Scalability Analysis
The architecture exhibits promising scalability:
• PE Scaling: Linear performance scaling with increasing PE count.
• Channel Scaling: Flexiblehandlingofinput/outputchannelswithproportionalresource
usage.
• Filter Size Flexibility: Native support for 3×3 filters, with potential for 1×1 and 5×5
filters.
3.5.4 Comparison with State-of-the-Art
Compared to other FPGA-based CNN accelerators, the quadrant-based design provides:
• High efficiency: Optimized for spatial resolution with reduced memory conflicts.
• Balanced resource utilization: Efficient use of on-chip memory and compute units.
• Low DSP usage: Design achieves competitive performance with only 8 DSP units com-
pared to other implementations.
15

=== Page 21 ===
Table 3.1: Quantitative Comparison with Prior Implementations [4]
Metric RAMAN[4] Eyeriss [1] Synetgy FiNN OurAccelerator
Platform XilinxZU3EG 65nmASIC XilinxZU3EG XilinxZynq-7020 Zedboard(Simulation)
Model MobileNetV1,DS-CNN AlexNet DiracDeltaNet iSmart2 CNN(3×3filters)
Precision 2,4or8b 16b W:1b,Act:4b N/A 8b
LUTs 37.2k N/A 24.13k 39.19k 39,534
DSPs 61 168PEs 37 220 8
Freq. (MHz) 75 100-250 250 150 100
Performance N/A 16.8-42.0GMACs N/A N/A 9.1GOPS*
PowerEfficiency 98.47GOPs/W(8b) N/A 8.56GOPs/W N/A N/A
Power(W) 137mW N/A 5.5 1.97 N/A
Reconfigurable
Row-stationary Binaryweight Flexible Quadrant-basedprocessing,
KeyFeatures accuracy,
dataflow,NoC network parallelism Windowbuffering
Energy-efficient
*Theoreticalpeakperformanceinsimulation
• Memory-centric approach: 93% BRAM utilization indicates an architecture optimized
for data movement rather than computation-intensive operations.
• Competitive performance: Particularly in early network layers.
• High efficiency: Optimized for spatial resolution with reduced memory conflicts.
• Balanced resource utilization: Efficient use of on-chip memory and compute units.
3.6 Conclusion
The proposed quadrant-based CNN accelerator demonstrates a highly effective strategy for
implementing convolutional neural networks on FPGA platforms. By combining:
• Hierarchical memory design
• Quadrant-based data partitioning
• Parallel and pipelined computation
The architecture achieves high computational throughput while remaining resource-efficient.
It is particularly well-suited for early CNN layers, as found in networks like MobileNetV2, where
large spatial dimensions and modest channel counts are common. Future enhancements could
focus on optimizing the architecture for deeper layers, which typically feature smaller spatial
dimensions and larger channel counts.
16

=== Page 22 ===
Chapter 4
Algorithm Implementation and Evaluation
4.1 Executive Summary
We use the TUB CrowdFlow synthetic optical-flow benchmark to develop and evaluate
a compact CNN+GRU model for classifying crowd motion patterns. The CustomCNN
encoder—three 3×3 Conv→BatchNorm→ReLU blocks with MaxPool and an Adap-
tiveAvgPool to 128-D embeddings—feeds into a single-layer GRU (hidden = 256, dropout
=0.5)andafinalLinearlayermappingtofivesceneclasses. TrainingusesCrossEntropyLoss
withtheAdamoptimizer(lr=0.001,weight decay=1e-4),mixedprecisionviatorch.cuda.amp
(autocasting + GradScaler), and early stopping (patience = 10) alongside extensive data aug-
mentation—resulting in ∼ 93% F1 score.
4.2 Dataset Description
TheTUB CrowdFlowdatasetfromTUBerlin’sCommunicationSystemsGroupcomprisesten
HD(1280×720)sequences—fivestatic-cameraandfiveUAV-simulateddynamicviews—captured
at 25 fps, totaling 3200 frames. Ground-truth annotations include dense optical-flow fields, per-
son trajectories (up to 1451 per scene), and dense pixel trajectories. This dataset is a standard
benchmark for evaluating crowd analytics under both fixed and moving viewpoints, providing
ground truth for optical flow and multi-object trajectory estimation.
4.3 Model Architecture
4.3.1 Custom CNN Encoder
The encoder consists of three sequential convolutional blocks:
• Block 1:
Conv2d(3→32, 3×3, stride=1, padding=1) → BatchNorm2d(32) → ReLU() → MaxPool2d(2)
• Block 2:
17

=== Page 23 ===
Conv2d(32→64, 3×3, stride=1, padding=1) → BatchNorm2d(64) → ReLU() → MaxPool2d(2)
• Block 3:
Conv2d(64→128, 3×3, stride=1, padding=1) → BatchNorm2d(128) → ReLU() →
AdaptiveAvgPool2d((1,1))
The resulting 128×1×1 tensor is flattened into a 128-D feature vector.
4.3.2 GRU Sequence Model
A single-layer GRU processes the sequence of 128-D embeddings:
• Input size: 128
• Hidden size: 256
• Num layers: 1
• Dropout: 0.5 (applied on inputs between time steps)
• Batch-first: True
The final time-step hidden state is passed through Dropout(0.5) and a Linear(256→5) layer
for five-class classification.
4.4 Training Methodology
4.4.1 Data Loading and Splitting
Sequences of length 10 frames are extracted via a sliding window (stride = 5) over each video.
Samples are labeled by scene class and split randomly into 80% training and 20% validation
sets.
4.4.2 Data Augmentation
Training transforms:
• RandomHorizontalFlip(p=0.5)
• ColorJitter (brightness, contrast, saturation)
18

=== Page 24 ===
• RandomAffine transforms
• Resize(224,224)
• ToTensor() + Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
Validation transforms:
Resize(224,224) → ToTensor() → Normalize(...)
4.4.3 Optimization and Regularization
• Loss: CrossEntropyLoss()
• Optimizer: Adam(lr=0.001, weight decay=1e-4)
• Scheduler: ReduceLROnPlateau(factor=0.1, patience=5)
• Mixed Precision: torch.cuda.amp.autocast() + GradScaler()
• Early Stopping: Patience = 10 epochs without validation-loss improvement; best model
checkpointed.
4.5 Results
The model achieves ∼ 93% F1 score. Training and validation loss curves converge with minimal
overfitting. Confusion matrices and classification reports confirm balanced precision, recall, and
F1-scores across all five classes.
19

=== Page 25 ===
Chapter 5
SoC Architecture
5.1 Current SoC Implementation
5.1.1 Foundational Infrastructure
• Ibex RISC-V Core
– Core Implementation: RV32IMC ISA, two-stage pipeline, three-cycle multiplier
– Memory Interface: AHB-Lite protocol to instruction and data memory
– Exception Handling: Full RISC-V standard interrupt controller support
– Debug Support: JTAG debug interface for on-chip debugging
5.1.2 Memory Subsystem
• Memory Architecture: 64KB Von-Neumann with 2KB stack
• ROM: Bootloader and initialization code
5.1.3 Peripheral Integration
• UART Controllers
– UART-1: System debugging and console
– UART-2: Application-specific communication
• I2C Controller: Master only, standard mode (100kbit/s)
• GPIO Interface: Configurable direction and interrupt capability
5.1.4 Interconnect System
• AHB-Lite Bus: Central AMBA 3 AHB-Lite interconnect
• Protocol Conversion: Wrapper between native Ibex protocol and AHB-Lite
• Address Decoding: Partitioned address space for memory and peripherals
20

=== Page 26 ===
Figure 5.1: Proposed SoC Architecture with AXI DMA Integration
5.2 Proposed SoC with AXI DMA Integration
5.2.1 AXI DMA Controller
• High-Bandwidth Transfers: Bursts up to 256 words, 4 outstanding transactions
• Memory–Accelerator Streaming:
– DDR → CNN accelerator output BRAMs
– Accelerator output BRAMs → DDR
• Protocol Bridges: AHB to AXI conversions for integration
5.2.2 Enhanced Interconnect Architecture
• AXI4 Bus: Connects DMA, CNN accelerator, DDR memory
• AHB–AXI Bridge: Protocol conversion between AHB-Lite and AXI4
5.2.3 Memory Subsystem Enhancements
• DDR Controller: External DDR frame buffer and weight storage
• ROM Connection: DMA access for code execution
• Memory Protection: RISC-V PMP regions for access control
• Memory Map Revision: Updated address map including DMA and accelerator
21

[Image page_26_image_0.jpeg Analysis (by Gemini)]
Here's a thorough analysis of the image, incorporating its visual elements, text, and context:

**1. Visual Elements and Structure:**

*   **Diagram Type:** The image is a block diagram illustrating the proposed System-on-Chip (SoC) architecture. It shows the major components and their interconnections.
*   **Shapes and Colors:** Rectangles represent the functional blocks (e.g., Ibex Core, UART controllers, DDR memory). Different colors are used to distinguish the blocks. Arrows indicate the data flow and connections between components. Specifically:
    *   Green: Ibex Core and related logic.
    *   Blue: Interconnect and bus structures (AHB-Lite Bus).
    *   Yellow: Peripheral controllers (UART, I2C, ROM).
    *   Orange: DMA and CNN accelerator related components.
    *   Red: External memory (DDR).
    *   Purple: PE Controller.
*   **Hierarchical Structure:** The diagram has a hierarchical structure. The core (Ibex Core) and peripherals are connected via the AHB-Lite Bus. The CNN accelerator components (PE Controller, Processing Elements - PEs, BRAMs, DDR controller) are separate. The DMA acts as a bridge, connecting DDR memory to the AHB-Lite bus.
*   **Flow of Data:** Arrows show the data flow between modules. The Ibex Core communicates with peripherals via the AHB-Lite bus. The DMA controller facilitates high-bandwidth data transfers between DDR memory and the CNN accelerator.

**2. Text within the Image:**

*   **Title:** "Full Proposed System on Chip Design"
*   **Core Components:**
    *   "Ibex Core"
    *   "AHB wrapper"
    *   "AHB-Lite Bus"
    *   "UART-1"
    *   "UART-2"
    *   "I2C"
    *   "ROM"
    *   "DMA"
    *   "DDR"
*   **CNN Accelerator Components:**
    *   "PE CONTROLLER"
    *   "INPUT BRAM" (repeated 4 times)
    *   "OUTPUT BRAM" (repeated 4 times)
    *   "PE" (Processing Element, repeated 16 times)
    *   "DDR TO BRAM CONTROLLER"

**3. Image Context and Importance:**

*   **Purpose:** The diagram illustrates the *proposed* SoC architecture with AXI DMA integration, as stated in the caption of Figure 5.1 on page 21. It's a conceptual representation of how different hardware components will be interconnected to enable high-performance data transfers for the CNN accelerator.
*   **Chapter Context:** This figure belongs to Chapter 5, which describes the SoC Architecture. Page 20 discusses the *current* SoC implementation, whereas page 21 presents the proposed *improvements* with DMA and a CNN accelerator. Page 22 discusses the software stack to support these new features.
*   **AXI DMA Integration:** The AXI DMA (Direct Memory Access) controller is a crucial component for enabling high-bandwidth data transfers between the DDR memory and the CNN accelerator's BRAMs (Block RAMs). This is essential for the CNN to operate efficiently, as it needs to access large amounts of data (e.g., input frames, weights) quickly.
*   **Significance:** The image highlights the system-level architecture that enables the CNN accelerator to efficiently process data stored in external DDR memory. The DMA offloads the data transfer burden from the Ibex core, freeing it up for other tasks. The integration involves hardware (AXI DMA, AXI4 bus) and software (DMA driver, accelerator API) modifications. This proposed architecture moves beyond the simpler AHB-Lite based system of the current implmentation detailed on page 20.

**In summary:** The diagram provides a visual overview of the planned architectural enhancements to the SoC, primarily focusing on the integration of an AXI DMA controller to facilitate high-bandwidth data transfer for a CNN accelerator. This proposed architecture aims to improve performance and efficiency compared to the current implementation. The diagram presents the connectivity and the functional blocks necessary to achieve this.



=== Page 27 ===
5.2.4 Software Stack Modifications
• DMA Driver: Low-level configuration and control
• Memory Management: Allocation of DMA-accessible regions
• Accelerator API: High-level CNN operations leveraging DMA
5.3 Implementation Status and Verification
5.3.1 Implementation Status
• Ibex Core & Peripherals: FPGA-prototyped, fully verified
• CNN Accelerator: HLS-implemented, synthesized, functionally verified
• AXI DMA Controller: Design complete; integration ongoing
• Interconnect System: AHB-Lite done; AXI4 under development
• Software Stack: Bootloader and peripheral drivers ready; DMA driver in progress
22

=== Page 28 ===
Chapter 6
Future Directions
• AcceleratorIntegration: ConnecttheCNNacceleratortotheSoCinterconnect(AHB/AXI)
for memory-mapped control and data access.
• DMA Engine Implementation: Finalize AXI DMA integration for efficient streaming
of feature maps and weights between DDR and the CNN IP.
• Complete Pipeline Validation: Execute the full CNN+LSTM inference pipeline on
silicon/FPGA to verify end-to-end functionality.
• Zero-Copy Buffer Management: Software framework for zero-copy sharing of buffers
between the Ibex core and CNN accelerator.
• CSC Format Support: Hardware support for compressed sparse column formats to
reduce off-chip bandwidth.
• Zero-Skipping Logic: Computeelementsthatdynamicallybypasszero-valuedoperands.
• PrecisionScaling: Configurabledata-pathwidths(8-bit,4-bit,2-bit)foraccuracy/throughput
trade-offs.
23

=== Page 29 ===
References
[1] Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An energy-efficient
reconfigurable accelerator for deep convolutional neural networks. IEEE Journal of Solid-
State Circuits 52, 1 (2017), 127–138.
[2] Chen, Y.-H., Yang, T.-J., Emer, J., and Sze, V. Eyeriss v2: A flexible accelerator for
emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected
Topics in Circuits and Systems 9, 2 (2019), 292–308.
[3] Gao, C., Rios-Navarro, A., Chen, X., Liu, S.-C., and Delbruck, T. Edgedrnn:
Recurrent neural network accelerator for edge inference. IEEE Journal on Emerging and
Selected Topics in Circuits and Systems 10, 4 (2020), 419–432.
[4] Krishna, A., Rohit Nudurupati, S., Chandana, D. G., Dwivedi, P., van Schaik,
A., Mehendale, M., and Thakur, C. S. Raman: A reconfigurable and sparse tinyml
accelerator for inference on edge. IEEE Internet of Things Journal 11, 14 (2024), 24831–
24845.
24

